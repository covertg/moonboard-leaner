{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moon.data\n",
    "import moon.problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = moon.data.read_problems('data/cleaned_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30991, 18, 11) (30991, 16)\n",
      "((24793, 18, 11), (24793, 16)) ((6198, 18, 11), (6198, 16))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([p.array for p in probs])\n",
    "y = np.array([p.grade.ordinal[1:] for p in probs])  # Convert \"ordinal\" to \"rank\"\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "split = 0.2\n",
    "indices = np.arange(len(probs))\n",
    "np.random.shuffle(indices)\n",
    "n_test = int(len(probs) * split)\n",
    "\n",
    "x_train, y_train = x[indices[n_test:]], y[indices[n_test:]]\n",
    "x_test, y_test = x[indices[0:n_test]], y[indices[0:n_test]]\n",
    "print((x_train.shape, y_train.shape), (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def accuracy_k_coral(k):\n",
    "    def fn(y_true, y_pred):\n",
    "        pred_rank = tf.reduce_sum(tf.cast(y_pred > 0.5, 'float32'), axis=-1)\n",
    "        true_rank = tf.reduce_sum(y_true, axis=-1)\n",
    "        score_bools = tf.abs(pred_rank - true_rank) <= k\n",
    "        return tf.reduce_mean(tf.cast(score_bools, 'float32'), axis=-1)\n",
    "    fn.__name__ = 'acc' if k == 0 else f'CS{k}'  # https://stackoverflow.com/questions/57910680/how-to-name-custom-metrics-in-keras-fit-output\n",
    "    return fn\n",
    "\n",
    "def mae_coral():\n",
    "    def fn(y_true, y_pred):\n",
    "        pred_rank = tf.reduce_sum(tf.cast(y_pred > 0.5, 'float32'), axis=-1)\n",
    "        true_rank = tf.reduce_sum(y_true, axis=-1)\n",
    "        dist_abs = tf.abs(pred_rank - true_rank)\n",
    "        return tf.reduce_mean(dist_abs, axis=-1)\n",
    "    fn.__name__ = 'mae'\n",
    "    return fn\n",
    "\n",
    "def rmse_coral():\n",
    "    def fn(y_true, y_pred):\n",
    "        pred_rank = tf.reduce_sum(tf.cast(y_pred > 0.5, 'float32'), axis=-1)\n",
    "        true_rank = tf.reduce_sum(y_true, axis=-1)\n",
    "        dist_sqr = (pred_rank - true_rank)**2\n",
    "        return tf.math.sqrt(tf.reduce_mean(dist_sqr, axis=-1))\n",
    "    fn.__name__ = 'rmse'\n",
    "    return fn\n",
    "\n",
    "# see\n",
    "# https://arxiv.org/abs/1901.07884 and https://github.com/Raschka-research-group/coral-cnn/issues/9\n",
    "# more background https://arxiv.org/abs/1705.05278\n",
    "\n",
    "class CoralOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_len):\n",
    "    super(CoralOutput, self).__init__()\n",
    "    self.output_len = output_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\n",
    "        'kernel',\n",
    "        shape=[int(input_shape[-1]), 1],\n",
    "        initializer='glorot_uniform',\n",
    "        dtype='float32',\n",
    "        trainable=True\n",
    "    )\n",
    "    self.biases = self.add_weight(\n",
    "        'biases',\n",
    "        shape=[self.output_len,],\n",
    "        initializer='zeros',\n",
    "        dtype='float32',\n",
    "        trainable=True\n",
    "    )\n",
    "\n",
    "  def call(self, input):\n",
    "    fc = tf.matmul(input, self.kernel)\n",
    "    fc = tf.tile(fc, [1, self.output_len])\n",
    "    logits = tf.nn.bias_add(fc, self.biases, name='logits')\n",
    "    probits = tf.math.sigmoid(logits, name='probits')\n",
    "    return logits, probits\n",
    "\n",
    "\n",
    "def _task_importance_weighting(y):\n",
    "    n_ranks = y.shape[-1]\n",
    "    ranks = 1 + np.sum(y, axis=-1)\n",
    "    n_examples = len(y)\n",
    "    m = np.zeros(n_ranks)\n",
    "    for k in range(n_ranks):\n",
    "        s_k = np.sum(ranks > (k + 1))\n",
    "        m[k] = np.sqrt(max(s_k, n_examples - s_k))\n",
    "    return (m / np.max(m))\n",
    "\n",
    "\n",
    "# # adapation of https://github.com/Raschka-research-group/coral-cnn/blob/master/model-code/resnet34/cacd-coral.py#L326\n",
    "# the authors describe this as \"weighted cross-entropy\" of the K-1 binary classifiers.\n",
    "# note (1) we calculate on the output logits, (2) the form in code differs from the form\n",
    "# in the paper for numerical stability, (3) importance weightings are TODO.\n",
    "def coral_loss(rank_data=None):\n",
    "    importance = 1 if rank_data is None else _task_importance_weighting(rank_data)\n",
    "    def loss_logits(y_true, y_pred):\n",
    "        unweighted = (tf.math.log_sigmoid(y_pred) * y_true) + (tf.math.log_sigmoid(y_pred) - y_pred) * (1 - y_true)\n",
    "        return tf.reduce_mean(-1 * tf.reduce_sum(importance * unweighted, axis=1))\n",
    "    return loss_logits\n",
    "\n",
    "# see\n",
    "# https://arxiv.org/abs/1901.07884 and https://github.com/Raschka-research-group/coral-cnn/issues/9\n",
    "# more background https://arxiv.org/abs/1705.05278\n",
    "\n",
    "class CoralOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_len):\n",
    "    super(CoralOutput, self).__init__()\n",
    "    self.output_len = output_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\n",
    "        'kernel',\n",
    "        shape=[int(input_shape[-1]), 1],\n",
    "        initializer='glorot_uniform',\n",
    "        dtype='float32',\n",
    "        trainable=True\n",
    "    )\n",
    "    self.biases = self.add_weight(\n",
    "        'biases',\n",
    "        shape=[self.output_len,],\n",
    "        initializer='zeros',\n",
    "        dtype='float32',\n",
    "        trainable=True\n",
    "    )\n",
    "\n",
    "  def call(self, input):\n",
    "    fc = tf.matmul(input, self.kernel)\n",
    "    fc = tf.tile(fc, [1, self.output_len])\n",
    "    logits = tf.nn.bias_add(fc, self.biases, name='logits')\n",
    "    probits = tf.math.sigmoid(logits, name='probits')\n",
    "    return logits, probits\n",
    "\n",
    "\n",
    "def _task_importance_weighting(y):\n",
    "    n_ranks = y.shape[-1]\n",
    "    ranks = 1 + np.sum(y, axis=-1)\n",
    "    n_examples = len(y)\n",
    "    m = np.zeros(n_ranks)\n",
    "    for k in range(n_ranks):\n",
    "        s_k = np.sum(ranks > (k + 1))\n",
    "        m[k] = np.sqrt(max(s_k, n_examples - s_k))\n",
    "    return (m / np.max(m))\n",
    "\n",
    "\n",
    "# # adapation of https://github.com/Raschka-research-group/coral-cnn/blob/master/model-code/resnet34/cacd-coral.py#L326\n",
    "# the authors describe this as \"weighted cross-entropy\" of the K-1 binary classifiers.\n",
    "# note (1) we calculate on the output logits, (2) the form in code differs from the form\n",
    "# in the paper for numerical stability, (3) importance weightings are TODO.\n",
    "def coral_loss(rank_data=None):\n",
    "    importance = 1 if rank_data is None else _task_importance_weighting(rank_data)\n",
    "    def loss_logits(y_true, y_pred):\n",
    "        unweighted = (tf.math.log_sigmoid(y_pred) * y_true) + (tf.math.log_sigmoid(y_pred) - y_pred) * (1 - y_true)\n",
    "        return tf.reduce_mean(-1 * tf.reduce_sum(importance * unweighted, axis=1))\n",
    "    return loss_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 8.9427 - logits_loss: 8.9427 - probits_acc: 0.0336 - probits_CS1: 0.0978 - probits_CS2: 0.1521 - probits_mae: 5.0060 - probits_rmse: 5.5387 - val_loss: 7.9623 - val_logits_loss: 7.9623 - val_probits_acc: 0.0659 - val_probits_CS1: 0.1957 - val_probits_CS2: 0.3120 - val_probits_mae: 3.6564 - val_probits_rmse: 4.2077\n",
      "Epoch 2/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 7.3016 - logits_loss: 7.3016 - probits_acc: 0.1268 - probits_CS1: 0.3533 - probits_CS2: 0.5212 - probits_mae: 2.7265 - probits_rmse: 3.3494 - val_loss: 6.6445 - val_logits_loss: 6.6445 - val_probits_acc: 0.1842 - val_probits_CS1: 0.4789 - val_probits_CS2: 0.6701 - val_probits_mae: 2.0435 - val_probits_rmse: 2.6396\n",
      "Epoch 3/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 6.1635 - logits_loss: 6.1635 - probits_acc: 0.2003 - probits_CS1: 0.5259 - probits_CS2: 0.7403 - probits_mae: 1.8127 - probits_rmse: 2.3850 - val_loss: 5.6951 - val_logits_loss: 5.6951 - val_probits_acc: 0.2248 - val_probits_CS1: 0.5977 - val_probits_CS2: 0.8099 - val_probits_mae: 1.5561 - val_probits_rmse: 2.0966\n",
      "Epoch 4/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 5.3397 - logits_loss: 5.3397 - probits_acc: 0.2258 - probits_CS1: 0.5977 - probits_CS2: 0.8145 - probits_mae: 1.5388 - probits_rmse: 2.0733 - val_loss: 4.9965 - val_logits_loss: 4.9965 - val_probits_acc: 0.2275 - val_probits_CS1: 0.6370 - val_probits_CS2: 0.8447 - val_probits_mae: 1.4327 - val_probits_rmse: 1.9354\n",
      "Epoch 5/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 4.7265 - logits_loss: 4.7265 - probits_acc: 0.2333 - probits_CS1: 0.6279 - probits_CS2: 0.8399 - probits_mae: 1.4406 - probits_rmse: 1.9465 - val_loss: 4.4704 - val_logits_loss: 4.4704 - val_probits_acc: 0.2406 - val_probits_CS1: 0.6531 - val_probits_CS2: 0.8497 - val_probits_mae: 1.3918 - val_probits_rmse: 1.8989\n",
      "Epoch 6/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 4.2625 - logits_loss: 4.2625 - probits_acc: 0.2469 - probits_CS1: 0.6488 - probits_CS2: 0.8502 - probits_mae: 1.3756 - probits_rmse: 1.8703 - val_loss: 4.0649 - val_logits_loss: 4.0649 - val_probits_acc: 0.2551 - val_probits_CS1: 0.6640 - val_probits_CS2: 0.8533 - val_probits_mae: 1.3528 - val_probits_rmse: 1.8589\n",
      "Epoch 7/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 3.9020 - logits_loss: 3.9020 - probits_acc: 0.2620 - probits_CS1: 0.6588 - probits_CS2: 0.8539 - probits_mae: 1.3383 - probits_rmse: 1.8369 - val_loss: 3.7445 - val_logits_loss: 3.7445 - val_probits_acc: 0.2661 - val_probits_CS1: 0.6700 - val_probits_CS2: 0.8570 - val_probits_mae: 1.3184 - val_probits_rmse: 1.8154\n",
      "Epoch 8/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 3.6206 - logits_loss: 3.6206 - probits_acc: 0.2755 - probits_CS1: 0.6676 - probits_CS2: 0.8582 - probits_mae: 1.3059 - probits_rmse: 1.8046 - val_loss: 3.4886 - val_logits_loss: 3.4886 - val_probits_acc: 0.2841 - val_probits_CS1: 0.6734 - val_probits_CS2: 0.8586 - val_probits_mae: 1.2870 - val_probits_rmse: 1.7850\n",
      "Epoch 9/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 3.3881 - logits_loss: 3.3881 - probits_acc: 0.2858 - probits_CS1: 0.6763 - probits_CS2: 0.8615 - probits_mae: 1.2765 - probits_rmse: 1.7762 - val_loss: 3.2808 - val_logits_loss: 3.2808 - val_probits_acc: 0.2902 - val_probits_CS1: 0.6805 - val_probits_CS2: 0.8620 - val_probits_mae: 1.2633 - val_probits_rmse: 1.7558\n",
      "Epoch 10/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 3.2015 - logits_loss: 3.2015 - probits_acc: 0.3006 - probits_CS1: 0.6786 - probits_CS2: 0.8639 - probits_mae: 1.2511 - probits_rmse: 1.7519 - val_loss: 3.1060 - val_logits_loss: 3.1060 - val_probits_acc: 0.3140 - val_probits_CS1: 0.6839 - val_probits_CS2: 0.8631 - val_probits_mae: 1.2309 - val_probits_rmse: 1.7355\n",
      "Epoch 11/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 3.0399 - logits_loss: 3.0399 - probits_acc: 0.3133 - probits_CS1: 0.6848 - probits_CS2: 0.8673 - probits_mae: 1.2224 - probits_rmse: 1.7243 - val_loss: 2.9632 - val_logits_loss: 2.9632 - val_probits_acc: 0.3180 - val_probits_CS1: 0.6863 - val_probits_CS2: 0.8631 - val_probits_mae: 1.2213 - val_probits_rmse: 1.7245\n",
      "Epoch 12/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.9114 - logits_loss: 2.9114 - probits_acc: 0.3199 - probits_CS1: 0.6860 - probits_CS2: 0.8686 - probits_mae: 1.2103 - probits_rmse: 1.7103 - val_loss: 2.8395 - val_logits_loss: 2.8395 - val_probits_acc: 0.3310 - val_probits_CS1: 0.6880 - val_probits_CS2: 0.8658 - val_probits_mae: 1.1999 - val_probits_rmse: 1.7061\n",
      "Epoch 13/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.7945 - logits_loss: 2.7945 - probits_acc: 0.3302 - probits_CS1: 0.6910 - probits_CS2: 0.8737 - probits_mae: 1.1850 - probits_rmse: 1.6846 - val_loss: 2.7352 - val_logits_loss: 2.7352 - val_probits_acc: 0.3398 - val_probits_CS1: 0.6889 - val_probits_CS2: 0.8682 - val_probits_mae: 1.1863 - val_probits_rmse: 1.6957\n",
      "Epoch 14/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.7034 - logits_loss: 2.7034 - probits_acc: 0.3315 - probits_CS1: 0.6920 - probits_CS2: 0.8729 - probits_mae: 1.1820 - probits_rmse: 1.6806 - val_loss: 2.6428 - val_logits_loss: 2.6428 - val_probits_acc: 0.3493 - val_probits_CS1: 0.6928 - val_probits_CS2: 0.8727 - val_probits_mae: 1.1645 - val_probits_rmse: 1.6740\n",
      "Epoch 15/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.6217 - logits_loss: 2.6217 - probits_acc: 0.3365 - probits_CS1: 0.6950 - probits_CS2: 0.8730 - probits_mae: 1.1720 - probits_rmse: 1.6720 - val_loss: 2.5615 - val_logits_loss: 2.5615 - val_probits_acc: 0.3491 - val_probits_CS1: 0.6947 - val_probits_CS2: 0.8780 - val_probits_mae: 1.1489 - val_probits_rmse: 1.6436\n",
      "Epoch 16/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.5463 - logits_loss: 2.5463 - probits_acc: 0.3434 - probits_CS1: 0.6981 - probits_CS2: 0.8765 - probits_mae: 1.1555 - probits_rmse: 1.6542 - val_loss: 2.4940 - val_logits_loss: 2.4940 - val_probits_acc: 0.3546 - val_probits_CS1: 0.6961 - val_probits_CS2: 0.8753 - val_probits_mae: 1.1490 - val_probits_rmse: 1.6554\n",
      "Epoch 17/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.4844 - logits_loss: 2.4844 - probits_acc: 0.3478 - probits_CS1: 0.6964 - probits_CS2: 0.8780 - probits_mae: 1.1484 - probits_rmse: 1.6458 - val_loss: 2.4304 - val_logits_loss: 2.4304 - val_probits_acc: 0.3608 - val_probits_CS1: 0.6994 - val_probits_CS2: 0.8792 - val_probits_mae: 1.1291 - val_probits_rmse: 1.6292\n",
      "Epoch 18/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.4260 - logits_loss: 2.4260 - probits_acc: 0.3551 - probits_CS1: 0.7009 - probits_CS2: 0.8805 - probits_mae: 1.1334 - probits_rmse: 1.6324 - val_loss: 2.3753 - val_logits_loss: 2.3753 - val_probits_acc: 0.3653 - val_probits_CS1: 0.7016 - val_probits_CS2: 0.8834 - val_probits_mae: 1.1141 - val_probits_rmse: 1.6087\n",
      "Epoch 19/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.3808 - logits_loss: 2.3808 - probits_acc: 0.3570 - probits_CS1: 0.6990 - probits_CS2: 0.8804 - probits_mae: 1.1298 - probits_rmse: 1.6250 - val_loss: 2.3417 - val_logits_loss: 2.3417 - val_probits_acc: 0.3660 - val_probits_CS1: 0.6960 - val_probits_CS2: 0.8773 - val_probits_mae: 1.1303 - val_probits_rmse: 1.6361\n",
      "Epoch 20/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.3386 - logits_loss: 2.3386 - probits_acc: 0.3556 - probits_CS1: 0.7023 - probits_CS2: 0.8805 - probits_mae: 1.1279 - probits_rmse: 1.6222 - val_loss: 2.2881 - val_logits_loss: 2.2881 - val_probits_acc: 0.3715 - val_probits_CS1: 0.7034 - val_probits_CS2: 0.8867 - val_probits_mae: 1.0988 - val_probits_rmse: 1.5913\n",
      "Epoch 21/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.3135 - logits_loss: 2.3135 - probits_acc: 0.3584 - probits_CS1: 0.7057 - probits_CS2: 0.8835 - probits_mae: 1.1190 - probits_rmse: 1.6172 - val_loss: 2.2527 - val_logits_loss: 2.2527 - val_probits_acc: 0.3747 - val_probits_CS1: 0.7050 - val_probits_CS2: 0.8849 - val_probits_mae: 1.0984 - val_probits_rmse: 1.5980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.2757 - logits_loss: 2.2757 - probits_acc: 0.3635 - probits_CS1: 0.7020 - probits_CS2: 0.8846 - probits_mae: 1.1139 - probits_rmse: 1.6077 - val_loss: 2.2243 - val_logits_loss: 2.2243 - val_probits_acc: 0.3766 - val_probits_CS1: 0.7028 - val_probits_CS2: 0.8843 - val_probits_mae: 1.0993 - val_probits_rmse: 1.6003\n",
      "Epoch 23/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.2493 - logits_loss: 2.2493 - probits_acc: 0.3618 - probits_CS1: 0.7007 - probits_CS2: 0.8852 - probits_mae: 1.1144 - probits_rmse: 1.6058 - val_loss: 2.1951 - val_logits_loss: 2.1951 - val_probits_acc: 0.3775 - val_probits_CS1: 0.7054 - val_probits_CS2: 0.8885 - val_probits_mae: 1.0903 - val_probits_rmse: 1.5879\n",
      "Epoch 24/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.2260 - logits_loss: 2.2260 - probits_acc: 0.3673 - probits_CS1: 0.7055 - probits_CS2: 0.8836 - probits_mae: 1.1063 - probits_rmse: 1.6032 - val_loss: 2.1736 - val_logits_loss: 2.1736 - val_probits_acc: 0.3807 - val_probits_CS1: 0.7087 - val_probits_CS2: 0.8869 - val_probits_mae: 1.0856 - val_probits_rmse: 1.5875\n",
      "Epoch 25/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.2032 - logits_loss: 2.2032 - probits_acc: 0.3666 - probits_CS1: 0.7073 - probits_CS2: 0.8873 - probits_mae: 1.1015 - probits_rmse: 1.5975 - val_loss: 2.1546 - val_logits_loss: 2.1546 - val_probits_acc: 0.3813 - val_probits_CS1: 0.7071 - val_probits_CS2: 0.8878 - val_probits_mae: 1.0848 - val_probits_rmse: 1.5844\n",
      "Epoch 26/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1785 - logits_loss: 2.1785 - probits_acc: 0.3684 - probits_CS1: 0.7087 - probits_CS2: 0.8876 - probits_mae: 1.0958 - probits_rmse: 1.5866 - val_loss: 2.1324 - val_logits_loss: 2.1324 - val_probits_acc: 0.3813 - val_probits_CS1: 0.7078 - val_probits_CS2: 0.8919 - val_probits_mae: 1.0767 - val_probits_rmse: 1.5692\n",
      "Epoch 27/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1616 - logits_loss: 2.1616 - probits_acc: 0.3718 - probits_CS1: 0.7121 - probits_CS2: 0.8864 - probits_mae: 1.0888 - probits_rmse: 1.5815 - val_loss: 2.1178 - val_logits_loss: 2.1178 - val_probits_acc: 0.3833 - val_probits_CS1: 0.7078 - val_probits_CS2: 0.8899 - val_probits_mae: 1.0769 - val_probits_rmse: 1.5732\n",
      "Epoch 28/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1484 - logits_loss: 2.1484 - probits_acc: 0.3692 - probits_CS1: 0.7090 - probits_CS2: 0.8897 - probits_mae: 1.0924 - probits_rmse: 1.5834 - val_loss: 2.1066 - val_logits_loss: 2.1066 - val_probits_acc: 0.3814 - val_probits_CS1: 0.7109 - val_probits_CS2: 0.8906 - val_probits_mae: 1.0751 - val_probits_rmse: 1.5689\n",
      "Epoch 29/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1397 - logits_loss: 2.1397 - probits_acc: 0.3720 - probits_CS1: 0.7081 - probits_CS2: 0.8895 - probits_mae: 1.0888 - probits_rmse: 1.5792 - val_loss: 2.0959 - val_logits_loss: 2.0959 - val_probits_acc: 0.3834 - val_probits_CS1: 0.7078 - val_probits_CS2: 0.8888 - val_probits_mae: 1.0782 - val_probits_rmse: 1.5748\n",
      "Epoch 30/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1253 - logits_loss: 2.1253 - probits_acc: 0.3705 - probits_CS1: 0.7121 - probits_CS2: 0.8871 - probits_mae: 1.0872 - probits_rmse: 1.5749 - val_loss: 2.0882 - val_logits_loss: 2.0882 - val_probits_acc: 0.3884 - val_probits_CS1: 0.7084 - val_probits_CS2: 0.8883 - val_probits_mae: 1.0720 - val_probits_rmse: 1.5710\n",
      "Epoch 31/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1150 - logits_loss: 2.1150 - probits_acc: 0.3738 - probits_CS1: 0.7104 - probits_CS2: 0.8901 - probits_mae: 1.0833 - probits_rmse: 1.5744 - val_loss: 2.0776 - val_logits_loss: 2.0776 - val_probits_acc: 0.3867 - val_probits_CS1: 0.7071 - val_probits_CS2: 0.8894 - val_probits_mae: 1.0728 - val_probits_rmse: 1.5685\n",
      "Epoch 32/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.1183 - logits_loss: 2.1183 - probits_acc: 0.3714 - probits_CS1: 0.7049 - probits_CS2: 0.8893 - probits_mae: 1.0924 - probits_rmse: 1.5819 - val_loss: 2.0696 - val_logits_loss: 2.0696 - val_probits_acc: 0.3865 - val_probits_CS1: 0.7073 - val_probits_CS2: 0.8899 - val_probits_mae: 1.0718 - val_probits_rmse: 1.5665\n",
      "Epoch 33/300\n",
      "388/388 [==============================] - 2s 5ms/step - loss: 2.1029 - logits_loss: 2.1029 - probits_acc: 0.3764 - probits_CS1: 0.7080 - probits_CS2: 0.8904 - probits_mae: 1.0826 - probits_rmse: 1.5726 - val_loss: 2.0650 - val_logits_loss: 2.0650 - val_probits_acc: 0.3873 - val_probits_CS1: 0.7112 - val_probits_CS2: 0.8907 - val_probits_mae: 1.0667 - val_probits_rmse: 1.5620\n",
      "Epoch 34/300\n",
      "388/388 [==============================] - 2s 5ms/step - loss: 2.1033 - logits_loss: 2.1033 - probits_acc: 0.3735 - probits_CS1: 0.7089 - probits_CS2: 0.8895 - probits_mae: 1.0867 - probits_rmse: 1.5772 - val_loss: 2.0561 - val_logits_loss: 2.0561 - val_probits_acc: 0.3854 - val_probits_CS1: 0.7147 - val_probits_CS2: 0.8917 - val_probits_mae: 1.0621 - val_probits_rmse: 1.5526\n",
      "Epoch 35/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0941 - logits_loss: 2.0941 - probits_acc: 0.3728 - probits_CS1: 0.7132 - probits_CS2: 0.8901 - probits_mae: 1.0805 - probits_rmse: 1.5682 - val_loss: 2.0519 - val_logits_loss: 2.0519 - val_probits_acc: 0.3858 - val_probits_CS1: 0.7144 - val_probits_CS2: 0.8928 - val_probits_mae: 1.0599 - val_probits_rmse: 1.5484\n",
      "Epoch 36/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0920 - logits_loss: 2.0920 - probits_acc: 0.3771 - probits_CS1: 0.7094 - probits_CS2: 0.8911 - probits_mae: 1.0805 - probits_rmse: 1.5728 - val_loss: 2.0477 - val_logits_loss: 2.0477 - val_probits_acc: 0.3897 - val_probits_CS1: 0.7113 - val_probits_CS2: 0.8911 - val_probits_mae: 1.0625 - val_probits_rmse: 1.5572\n",
      "Epoch 37/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0857 - logits_loss: 2.0857 - probits_acc: 0.3769 - probits_CS1: 0.7107 - probits_CS2: 0.8899 - probits_mae: 1.0789 - probits_rmse: 1.5709 - val_loss: 2.0446 - val_logits_loss: 2.0446 - val_probits_acc: 0.3885 - val_probits_CS1: 0.7113 - val_probits_CS2: 0.8915 - val_probits_mae: 1.0646 - val_probits_rmse: 1.5611\n",
      "Epoch 38/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0825 - logits_loss: 2.0825 - probits_acc: 0.3743 - probits_CS1: 0.7086 - probits_CS2: 0.8917 - probits_mae: 1.0811 - probits_rmse: 1.5672 - val_loss: 2.0373 - val_logits_loss: 2.0373 - val_probits_acc: 0.3897 - val_probits_CS1: 0.7134 - val_probits_CS2: 0.8927 - val_probits_mae: 1.0588 - val_probits_rmse: 1.5523\n",
      "Epoch 39/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0771 - logits_loss: 2.0771 - probits_acc: 0.3712 - probits_CS1: 0.7078 - probits_CS2: 0.8903 - probits_mae: 1.0858 - probits_rmse: 1.5697 - val_loss: 2.0362 - val_logits_loss: 2.0362 - val_probits_acc: 0.3890 - val_probits_CS1: 0.7127 - val_probits_CS2: 0.8925 - val_probits_mae: 1.0605 - val_probits_rmse: 1.5546\n",
      "Epoch 40/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0787 - logits_loss: 2.0787 - probits_acc: 0.3736 - probits_CS1: 0.7118 - probits_CS2: 0.8905 - probits_mae: 1.0802 - probits_rmse: 1.5667 - val_loss: 2.0308 - val_logits_loss: 2.0308 - val_probits_acc: 0.3930 - val_probits_CS1: 0.7134 - val_probits_CS2: 0.8930 - val_probits_mae: 1.0538 - val_probits_rmse: 1.5477\n",
      "Epoch 41/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0751 - logits_loss: 2.0751 - probits_acc: 0.3737 - probits_CS1: 0.7096 - probits_CS2: 0.8908 - probits_mae: 1.0804 - probits_rmse: 1.5658 - val_loss: 2.0272 - val_logits_loss: 2.0272 - val_probits_acc: 0.3876 - val_probits_CS1: 0.7138 - val_probits_CS2: 0.8915 - val_probits_mae: 1.0604 - val_probits_rmse: 1.5518\n",
      "Epoch 42/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0719 - logits_loss: 2.0719 - probits_acc: 0.3774 - probits_CS1: 0.7097 - probits_CS2: 0.8914 - probits_mae: 1.0780 - probits_rmse: 1.5673 - val_loss: 2.0306 - val_logits_loss: 2.0306 - val_probits_acc: 0.3913 - val_probits_CS1: 0.7113 - val_probits_CS2: 0.8917 - val_probits_mae: 1.0604 - val_probits_rmse: 1.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "388/388 [==============================] - 2s 4ms/step - loss: 2.0695 - logits_loss: 2.0695 - probits_acc: 0.3782 - probits_CS1: 0.7114 - probits_CS2: 0.8910 - probits_mae: 1.0737 - probits_rmse: 1.5613 - val_loss: 2.0231 - val_logits_loss: 2.0231 - val_probits_acc: 0.3891 - val_probits_CS1: 0.7160 - val_probits_CS2: 0.8925 - val_probits_mae: 1.0549 - val_probits_rmse: 1.5448\n",
      "Epoch 44/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0646 - logits_loss: 2.0646 - probits_acc: 0.3760 - probits_CS1: 0.7130 - probits_CS2: 0.8937 - probits_mae: 1.0725 - probits_rmse: 1.5596 - val_loss: 2.0228 - val_logits_loss: 2.0228 - val_probits_acc: 0.3879 - val_probits_CS1: 0.7176 - val_probits_CS2: 0.8927 - val_probits_mae: 1.0552 - val_probits_rmse: 1.5460\n",
      "Epoch 45/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0646 - logits_loss: 2.0646 - probits_acc: 0.3747 - probits_CS1: 0.7122 - probits_CS2: 0.8931 - probits_mae: 1.0738 - probits_rmse: 1.5566 - val_loss: 2.0245 - val_logits_loss: 2.0245 - val_probits_acc: 0.3934 - val_probits_CS1: 0.7110 - val_probits_CS2: 0.8917 - val_probits_mae: 1.0576 - val_probits_rmse: 1.5535\n",
      "Epoch 46/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0631 - logits_loss: 2.0631 - probits_acc: 0.3708 - probits_CS1: 0.7119 - probits_CS2: 0.8910 - probits_mae: 1.0796 - probits_rmse: 1.5611 - val_loss: 2.0212 - val_logits_loss: 2.0212 - val_probits_acc: 0.3933 - val_probits_CS1: 0.7183 - val_probits_CS2: 0.8935 - val_probits_mae: 1.0484 - val_probits_rmse: 1.5421\n",
      "Epoch 47/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0598 - logits_loss: 2.0598 - probits_acc: 0.3756 - probits_CS1: 0.7108 - probits_CS2: 0.8929 - probits_mae: 1.0748 - probits_rmse: 1.5586 - val_loss: 2.0202 - val_logits_loss: 2.0202 - val_probits_acc: 0.3928 - val_probits_CS1: 0.7141 - val_probits_CS2: 0.8915 - val_probits_mae: 1.0559 - val_probits_rmse: 1.5519\n",
      "Epoch 48/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0627 - logits_loss: 2.0627 - probits_acc: 0.3773 - probits_CS1: 0.7111 - probits_CS2: 0.8926 - probits_mae: 1.0733 - probits_rmse: 1.5602 - val_loss: 2.0178 - val_logits_loss: 2.0178 - val_probits_acc: 0.3903 - val_probits_CS1: 0.7185 - val_probits_CS2: 0.8935 - val_probits_mae: 1.0492 - val_probits_rmse: 1.5374\n",
      "Epoch 49/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0665 - logits_loss: 2.0665 - probits_acc: 0.3753 - probits_CS1: 0.7120 - probits_CS2: 0.8913 - probits_mae: 1.0746 - probits_rmse: 1.5582 - val_loss: 2.0175 - val_logits_loss: 2.0175 - val_probits_acc: 0.3955 - val_probits_CS1: 0.7157 - val_probits_CS2: 0.8938 - val_probits_mae: 1.0491 - val_probits_rmse: 1.5455\n",
      "Epoch 50/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0561 - logits_loss: 2.0561 - probits_acc: 0.3792 - probits_CS1: 0.7096 - probits_CS2: 0.8926 - probits_mae: 1.0723 - probits_rmse: 1.5596 - val_loss: 2.0155 - val_logits_loss: 2.0155 - val_probits_acc: 0.3913 - val_probits_CS1: 0.7168 - val_probits_CS2: 0.8936 - val_probits_mae: 1.0517 - val_probits_rmse: 1.5440\n",
      "Epoch 51/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0472 - logits_loss: 2.0472 - probits_acc: 0.3778 - probits_CS1: 0.7151 - probits_CS2: 0.8945 - probits_mae: 1.0644 - probits_rmse: 1.5447 - val_loss: 2.0171 - val_logits_loss: 2.0171 - val_probits_acc: 0.3854 - val_probits_CS1: 0.7152 - val_probits_CS2: 0.8941 - val_probits_mae: 1.0578 - val_probits_rmse: 1.5439\n",
      "Epoch 52/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0548 - logits_loss: 2.0548 - probits_acc: 0.3770 - probits_CS1: 0.7120 - probits_CS2: 0.8932 - probits_mae: 1.0724 - probits_rmse: 1.5585 - val_loss: 2.0139 - val_logits_loss: 2.0139 - val_probits_acc: 0.3897 - val_probits_CS1: 0.7164 - val_probits_CS2: 0.8944 - val_probits_mae: 1.0511 - val_probits_rmse: 1.5390\n",
      "Epoch 53/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0451 - logits_loss: 2.0451 - probits_acc: 0.3789 - probits_CS1: 0.7149 - probits_CS2: 0.8950 - probits_mae: 1.0635 - probits_rmse: 1.5467 - val_loss: 2.0127 - val_logits_loss: 2.0127 - val_probits_acc: 0.3925 - val_probits_CS1: 0.7156 - val_probits_CS2: 0.8943 - val_probits_mae: 1.0505 - val_probits_rmse: 1.5423\n",
      "Epoch 54/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0556 - logits_loss: 2.0556 - probits_acc: 0.3805 - probits_CS1: 0.7106 - probits_CS2: 0.8941 - probits_mae: 1.0691 - probits_rmse: 1.5553 - val_loss: 2.0149 - val_logits_loss: 2.0149 - val_probits_acc: 0.3941 - val_probits_CS1: 0.7138 - val_probits_CS2: 0.8930 - val_probits_mae: 1.0529 - val_probits_rmse: 1.5479\n",
      "Epoch 55/300\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 2.0470 - logits_loss: 2.0470 - probits_acc: 0.3780 - probits_CS1: 0.7130 - probits_CS2: 0.8946 - probits_mae: 1.0673 - probits_rmse: 1.5499 - val_loss: 2.0127 - val_logits_loss: 2.0127 - val_probits_acc: 0.3939 - val_probits_CS1: 0.7154 - val_probits_CS2: 0.8935 - val_probits_mae: 1.0502 - val_probits_rmse: 1.5438\n",
      "Epoch 56/300\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 2.0464 - logits_loss: 2.0464 - probits_acc: 0.3785 - probits_CS1: 0.7118 - probits_CS2: 0.8934 - probits_mae: 1.0683 - probits_rmse: 1.5502 - val_loss: 2.0134 - val_logits_loss: 2.0134 - val_probits_acc: 0.3918 - val_probits_CS1: 0.7149 - val_probits_CS2: 0.8949 - val_probits_mae: 1.0516 - val_probits_rmse: 1.5428\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "p = .5\n",
    "input_shape = moon.problem.Problem.GRID_SHAPE\n",
    "hiddens = [20]\n",
    "hidden_activation = 'swish'\n",
    "output_len = y.shape[-1]  # moon.problem.Grade.N_GRADES - 1\n",
    "output_activation = 'sigmoid'\n",
    "\n",
    "metrics = {'probits': [accuracy_k_coral(0), accuracy_k_coral(1), accuracy_k_coral(2), mae_coral(), rmse_coral()]}\n",
    "loss = {'logits': coral_loss(rank_data=y)}\n",
    "adam_lr = 1e-3\n",
    "optim = tf.keras.optimizers.Adam(lr=adam_lr)\n",
    "\n",
    "in_x = layers.Input(shape=input_shape)\n",
    "features = layers.Flatten()(in_x)\n",
    "for nodes in hiddens:\n",
    "    features = layers.Dense(nodes)(features)\n",
    "    features = layers.Activation(hidden_activation)(features)\n",
    "    if p > 0: features = layers.Dropout(p)(features)\n",
    "out = CoralOutput(output_len)(features)\n",
    "logits, probits = layers.Lambda(lambda x: x, name='logits')(out[0]), layers.Lambda(lambda x: x, name='probits')(out[1])\n",
    "\n",
    "model = tf.keras.Model(in_x, [logits, probits], name='mlp_ord_regress')\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "# model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)]\n",
    "batch_size = 64\n",
    "max_epochs = 300\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_test, y_test), epochs=max_epochs, callbacks=callbacks)\n",
    "\n",
    "# [64. 64], swish, 1e-4, weighted : e222\n",
    "# loss: 1.9850 - logits_loss: 1.9850 - probits_acc: 0.3914 - probits_CS1: 0.7283 - probits_CS2: 0.9021 - probits_mae: 1.0237 - probits_rmse: 1.4996 - val_loss: 1.9964 - val_logits_loss: 1.9964 - val_probits_acc: 0.4009 - val_probits_CS1: 0.7228 - val_probits_CS2: 0.8974 - val_probits_mae: 1.0311 - val_probits_rmse: 1.5261\n",
    "\n",
    "# [32, 32], swish, 1e-4, unweighted\n",
    "# loss: 2.3904 - logits_loss: 2.3904 - probits_acc: 0.3860 - probits_CS1: 0.7195 - probits_CS2: 0.8967 - val_loss: 2.3429 - val_logits_loss: 2.3429 - val_probits_acc: 0.4045 - val_probits_CS1: 0.7236 - val_probits_CS2: 0.8988\n",
    "\n",
    "# [32, 32], swish, 1e-3, weighted : e69\n",
    "# loss: 1.9615 - logits_loss: 1.9615 - probits_acc: 0.3893 - probits_CS1: 0.7298 - probits_CS2: 0.9044 - val_loss: 1.9623 - val_logits_loss: 1.9623 - val_probits_acc: 0.4011 - val_probits_CS1: 0.7304 - val_probits_CS2: 0.9017\n",
    "\n",
    "# [32, 32], swish, 1e-3, unweighted : e59\n",
    "# loss: 2.3329 - logits_loss: 2.3329 - probits_acc: 0.3941 - probits_CS1: 0.7271 - probits_CS2: 0.9024 - val_loss: 2.3219 - val_logits_loss: 2.3219 - val_probits_acc: 0.3998 - val_probits_CS1: 0.7296 - val_probits_CS2: 0.9029\n",
    "\n",
    "# [32, 32], relu, 1e-3, weighted : e39\n",
    "# loss: 1.9442 - logits_loss: 1.9442 - probits_acc: 0.3801 - probits_CS1: 0.7346 - probits_CS2: 0.9086 - val_loss: 2.0306 - val_logits_loss: 2.0306 - val_probits_acc: 0.3871 - val_probits_CS1: 0.7162 - val_probits_CS2: 0.8927\n",
    "\n",
    "# [20], swish, 1e-3, weighted : e56\n",
    "# loss: 2.0464 - logits_loss: 2.0464 - probits_acc: 0.3785 - probits_CS1: 0.7118 - probits_CS2: 0.8934 - probits_mae: 1.0683 - probits_rmse: 1.5502 - val_loss: 2.0134 - val_logits_loss: 2.0134 - val_probits_acc: 0.3918 - val_probits_CS1: 0.7149 - val_probits_CS2: 0.8949 - val_probits_mae: 1.0516 - val_probits_rmse: 1.5428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'logits_loss', 'probits_acc', 'probits_CS1', 'probits_CS2', 'probits_mae', 'probits_rmse', 'val_loss', 'val_logits_loss', 'val_probits_acc', 'val_probits_CS1', 'val_probits_CS2', 'val_probits_mae', 'val_probits_rmse'])\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())  # stats\n",
    "print(len(history.history['loss']))  # epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11  8  5  8 14  6]\n",
      "[ 6  9  8  5  7 11  5]\n",
      "[[9.9937040e-01 9.9937040e-01 9.9937040e-01 9.9930549e-01 5.3878462e-01\n",
      "  3.3787346e-01 1.5452674e-01 5.6651533e-02 2.0606816e-02 1.0650635e-02\n",
      "  3.8231611e-03 1.0341406e-03 3.6516786e-04 1.3330579e-04 8.4384490e-05\n",
      "  5.7297973e-05]\n",
      " [9.9996597e-01 9.9996597e-01 9.9996597e-01 9.9996245e-01 9.5578682e-01\n",
      "  9.0424240e-01 7.7180529e-01 5.2636230e-01 2.8024450e-01 1.6612193e-01\n",
      "  6.6311300e-02 1.8797606e-02 6.7144930e-03 2.4591386e-03 1.5591383e-03\n",
      "  1.0591745e-03]\n",
      " [9.9992853e-01 9.9992853e-01 9.9992853e-01 9.9992108e-01 9.1147768e-01\n",
      "  8.1810713e-01 6.1699939e-01 3.4611639e-01 1.5644103e-01 8.6663932e-02\n",
      "  3.2720566e-02 9.0423524e-03 3.2094121e-03 1.1727810e-03 7.4315071e-04\n",
      "  5.0482154e-04]\n",
      " [9.9895829e-01 9.9895829e-01 9.9895829e-01 9.9885082e-01 4.1373944e-01\n",
      "  2.3563454e-01 9.9435955e-02 3.5009652e-02 1.2551397e-02 6.4615309e-03\n",
      "  2.3131669e-03 6.2501431e-04 2.2062659e-04 8.0475562e-05 5.0978546e-05\n",
      "  3.4614979e-05]\n",
      " [9.9987292e-01 9.9987292e-01 9.9987292e-01 9.9985981e-01 8.5277271e-01\n",
      "  7.1672612e-01 4.7540382e-01 2.2944456e-01 9.4469279e-02 5.0672889e-02\n",
      "  1.8673867e-02 5.1068962e-03 1.8080175e-03 6.6003203e-04 4.1824579e-04\n",
      "  2.8407574e-04]\n",
      " [9.9999595e-01 9.9999595e-01 9.9999595e-01 9.9999553e-01 9.9450743e-01\n",
      "  9.8751450e-01 9.6590382e-01 9.0298975e-01 7.6532483e-01 6.2527096e-01\n",
      "  3.7298441e-01 1.3827357e-01 5.3585529e-02 2.0229667e-02 1.2910515e-02\n",
      "  8.8028908e-03]\n",
      " [9.9750245e-01 9.9750245e-01 9.9750245e-01 9.9724519e-01 2.2716394e-01\n",
      "  1.1378682e-01 4.3966085e-02 1.4885634e-02 5.2662194e-03 2.7014613e-03\n",
      "  9.6467137e-04 2.6047230e-04 9.1914044e-05 3.3518103e-05 2.1232605e-05\n",
      "  1.4417167e-05]]\n"
     ]
    }
   ],
   "source": [
    "y_true = y_test[[2, 4, 8, 16, 32, 64, 128]]\n",
    "y_pred = model.predict(x_test[[2, 4, 8, 16, 32, 64, 128]])\n",
    "\n",
    "print(1+np.sum(y_true, axis=-1))\n",
    "print(1+np.sum(y_pred[1] > .5, axis=-1))\n",
    "print(y_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         0.99998387 0.99988706 0.81527716 0.73880707\n",
      " 0.77520879 0.85887471 0.91571489 0.94216616 0.96967674 0.98878876\n",
      " 0.99519675 0.99830452 0.9991284  0.99959658]\n"
     ]
    }
   ],
   "source": [
    "print(_task_importance_weighting(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python38264bitvenvvenv62f2ef13a99a49ca88eefe3bb4a02605"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
